#!/bin/bash

# üöÄ Setup FINAL Score SDE PyTorch pour GH200 ARM64
# Script test√© et valid√© - Version finale fonctionnelle
# Bas√© sur l'installation r√©elle r√©ussie

set -e

echo "üéØ Score SDE PyTorch - Setup FINAL pour GH200 ARM64"
echo "====================================================="
echo "GPU: NVIDIA GH200 480GB (94GB VRAM utilisable)"
echo "CUDA: 12.8"
echo "Architecture: ARM64"
echo "Ubuntu: 22.04"
echo ""

# D√©tection automatique de la configuration GPU
echo "üîç D√©tection de la configuration syst√®me..."
if command -v nvidia-smi &> /dev/null; then
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    echo "‚úÖ $GPU_COUNT GPU(s) d√©tect√©(s):"
    nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv,noheader,nounits | nl -w2 -s': ' | while read line; do
        echo "   ‚Üí GPU $line"
    done
    
    COMPUTE_CAPS=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader,nounits | sort -u | tr '\n' ';' | sed 's/;$//')
    echo "‚úÖ Compute Capabilities: $COMPUTE_CAPS"
else
    echo "‚ö†Ô∏è  nvidia-smi non disponible"
    GPU_COUNT=1
    COMPUTE_CAPS="9.0"
fi

# ===========================================
# √âTAPE 1: D√âPENDANCES SYST√àME
# ===========================================

echo ""
echo "üîß √âTAPE 1: Installation des d√©pendances syst√®me..."

sudo apt update
sudo apt install -y build-essential cmake ninja-build python3-dev pybind11-dev wget unzip

echo "‚úÖ D√©pendances syst√®me install√©es"

# ===========================================
# √âTAPE 2: D√âPENDANCES PYTHON ESSENTIELLES
# ===========================================

echo ""
echo "üì¶ √âTAPE 2: Installation d√©pendances Python..."

# Installation des packages manquants identifi√©s lors des tests
pip install tqdm ml-collections absl-py gdown pytorch-fid Pillow scipy pybind11
pip install tensorflow-datasets tensorflow-hub
pip install clean-fid
pip install seaborn
pip install questionary


# CRITIQUE: Downgrade NumPy pour compatibilit√© extensions CUDA
echo "üîß Downgrade NumPy pour compatibilit√©..."
pip install "numpy<2"

echo "‚úÖ D√©pendances Python install√©es"

# ===========================================
# √âTAPE 3: COMPILATION EXTENSIONS CUDA
# ===========================================

echo ""
echo "üî® √âTAPE 3: Compilation extensions CUDA pour GH200..."

if [ -d "op" ]; then
    cd op
    
    # Configuration variables CUDA pour GH200
    export CUDA_HOME="/usr/local/cuda"
    export TORCH_CUDA_ARCH_LIST="9.0"  # GH200 Compute Capability
    export FORCE_CUDA="1"
    
    echo "‚úÖ Variables CUDA configur√©es:"
    echo "   CUDA_HOME: $CUDA_HOME"
    echo "   TORCH_CUDA_ARCH_LIST: $TORCH_CUDA_ARCH_LIST"
    
    # Nettoyage complet
    rm -rf build/ *.so __pycache__/ 2>/dev/null || true
    
    # Compilation
    echo "  ‚Üí Compilation pour Compute Capability 9.0 (GH200)..."
    python setup.py build_ext --inplace
    
    cd ..
    echo "‚úÖ Extensions CUDA compil√©es"
else
    echo "‚ùå Dossier 'op' non trouv√©"
    exit 1
fi

# ===========================================
# √âTAPE 4: T√âL√âCHARGEMENT CHECKPOINTS
# ===========================================

echo ""
echo "üì¶ √âTAPE 4: T√©l√©chargement et installation checkpoints..."

# 4.1 Structure dossiers
mkdir -p experiments/{church,celebahq_256,ffhq_1024,afhq_512}_ncsnpp_continuous/{checkpoints,checkpoints-meta}

## 4.2 Church checkpoint
#echo "  ‚Üí Church checkpoint..."
#gdown --folder "https://drive.google.com/drive/folders/1zVChA0HrnJU66Jkt4P6KOnlREhBMc4Yh" --quiet 2>/dev/null || echo "    ‚ö†Ô∏è Church √©chou√©"
#if [ -d "church_ncsnpp_continuous" ]; then
#    find church_ncsnpp_continuous -name "*.pth" -exec cp {} experiments/church_ncsnpp_continuous/checkpoints-meta/checkpoint.pth \; 2>/dev/null || true
#    find church_ncsnpp_continuous -name "*.pth" -exec cp {} experiments/church_ncsnpp_continuous/checkpoints/ \; 2>/dev/null || true
#    rm -rf church_ncsnpp_continuous/
#    echo "    ‚úÖ Church install√©"
#fi


# 4.3 AFHQ checkpoints (TOUS les checkpoints)
echo "  ‚Üí AFHQ checkpoints..."
gdown --folder "https://drive.google.com/drive/folders/1xSoDpHiZSoBToPlJ2rdu2t2fXzzIQLQe" --quiet 2>/dev/null || echo "    ‚ö†Ô∏è AFHQ √©chou√©"

if [ -d "afhq_512_ncsnpp_continuous" ]; then
    mkdir -p experiments/afhq_512_ncsnpp_continuous/checkpoints/
    
    # Installer TOUS les checkpoints avec leurs noms d'origine
    for checkpoint_file in afhq_512_ncsnpp_continuous/checkpoint*.pth; do
        if [ -f "$checkpoint_file" ]; then
            filename=$(basename "$checkpoint_file")
            cp "$checkpoint_file" "experiments/afhq_512_ncsnpp_continuous/checkpoints/$filename"
            echo "    ‚úÖ Install√©: $filename"
            
            # Extraire le num√©ro d'it√©ration
            if [[ "$filename" =~ checkpoint_([0-9]+)\.pth ]]; then
                iter_num=${BASH_REMATCH[1]}
                iterations=$((iter_num * 20000))
                echo "       ‚Üí It√©rations: ${iterations:0}"
            elif [ "$filename" = "checkpoint.pth" ]; then
                echo "       ‚Üí It√©rations: 520000 (d√©faut)"
            fi
        fi
    done
    
    # Copier le checkpoint par d√©faut vers checkpoints-meta
    if [ -f "afhq_512_ncsnpp_continuous/checkpoint.pth" ]; then
        cp "afhq_512_ncsnpp_continuous/checkpoint.pth" experiments/afhq_512_ncsnpp_continuous/checkpoints-meta/checkpoint.pth
        echo "    ‚úÖ Checkpoint par d√©faut (520k iter) install√©"
    fi
    
    rm -rf afhq_512_ncsnpp_continuous/
fi

# 4.4 FFHQ_1024 et Celeab_256 checkpoints
#echo "  ‚Üí FFHQ_1024 et Celeba_256 checkpoints..."
#for model in "celebahq_256_ncsnpp_continuous:19VJ7UZTE-ytGX6z5rl-tumW9c0Ps3itk" "ffhq_1024_ncsnpp_continuous:1ZqLNr_kH0o9DxvwSlrQPMmkrhEnXhBm2"; do
#    name=$(echo $model | cut -d: -f1)
#    id=$(echo $model | cut -d: -f2)
    
#    gdown --folder "https://drive.google.com/drive/folders/$id" --quiet 2>/dev/null || continue
    
#    if [ -d "$name" ]; then
#        find "$name" -name "*.pth" -exec cp {} "experiments/$name/checkpoints-meta/checkpoint.pth" \; 2>/dev/null || true
#        find "$name" -name "*.pth" -exec cp {} "experiments/$name/checkpoints/" \; 2>/dev/null || true
#        rm -rf "$name/"
#        echo "    ‚úÖ $name install√©"
#    fi
#done


# 4.5 AFHQ classifier checkpoint
echo "  ‚Üí AFHQ classifier checkpoint..."
gdown --folder "https://drive.google.com/drive/u/0/folders/1p7l_4MHG7gAzdnc8oq2haORZCtHd2QN1" --quiet 2>/dev/null || echo "    ‚ö†Ô∏è AFHQ classifier √©chou√©"
if [ -d "afhq_classifier" ]; then

    if [ -f "afhq_classifier/afhq_classifier.pth" ]; then
        cp afhq_classifier/afhq_classifier.pth experiments/afhq_classifier/afhq_classifier.pth
        echo "    ‚úÖ AFHQ classifier 1 install√©"
    fi
    if [ -f "afhq_classifier/afhq_classifier_2.pth" ]; then
        cp afhq_classifier/afhq_classifier_2.pth experiments/afhq_classifier/afhq_classifier_2.pth
        echo "    ‚úÖ AFHQ classifier 2 install√©"
    fi
    rm -rf afhq_classifier/
fi


# ===========================================
# √âTAPE 5: DATASET AFHQ
# ===========================================

echo ""
echo "üêï √âTAPE 5: Installation dataset AFHQ..."

mkdir -p data
cd data/

if [ ! -d "afhq" ]; then
    echo "  ‚Üí T√©l√©chargement AFHQ (1.5GB)..."
    wget https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip --progress=bar --timeout=120 2>/dev/null || {
        echo "    ‚ö†Ô∏è T√©l√©chargement AFHQ √©chou√©"
        echo "    üí° T√©l√©chargez manuellement si n√©cessaire"
    }
    
    if [ -f "afhq.zip" ]; then
        echo "  ‚Üí Extraction..."
        unzip -q afhq.zip && rm afhq.zip
        echo "    ‚úÖ AFHQ dataset extrait"
    fi
else
    echo "  ‚úÖ Dataset AFHQ d√©j√† pr√©sent"
fi

cd ..





# ===========================================
# √âTAPE 6: TESTS DE VALIDATION
# ===========================================

echo ""
echo "üß™ √âTAPE 6: Tests de validation..."

echo "  ‚Üí Test environnement GPU..."
python3 -c "
import torch
print(f'‚úÖ PyTorch: {torch.__version__}')
print(f'‚úÖ CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'‚úÖ GPU Count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'‚úÖ GPU {i}: {torch.cuda.get_device_name(i)}')
        print(f'‚úÖ VRAM: {torch.cuda.get_device_properties(i).total_memory // 1024**3}GB')
" 2>/dev/null || echo "‚ö†Ô∏è  Test Python √©chou√©"

echo "  ‚Üí Test imports projet..."
python3 -c "
try:
    from op import upfirdn2d
    from op.fused_act import fused_leaky_relu, FusedLeakyReLU
    print('‚úÖ Extensions CUDA OK')
except Exception as e:
    print(f'‚ö†Ô∏è  Extensions CUDA: {e}')

try:
    from models import ncsnpp, ddpm, ncsnv2
    print('‚úÖ Models import OK')
except Exception as e:
    print(f'‚ùå Models error: {e}')

try:
    from configs.ve.afhq_512_ncsnpp_continuous import get_config
    config = get_config()
    print(f'‚úÖ Config auto-d√©tection: {config.training.batch_size} batch size')
except Exception as e:
    print(f'‚ùå Config error: {e}')
" 2>/dev/null || echo "‚ö†Ô∏è  Test imports √©chou√©"

# ===========================================
# V√âRIFICATION FINALE
# ===========================================

echo ""
echo "üîç V√âRIFICATION FINALE..."

# Checkpoints
echo "üì¶ Checkpoints install√©s:"
for checkpoint in "church" "afhq_512" "celebahq_256" "ffhq_1024"; do
    if [ -f "experiments/${checkpoint}_ncsnpp_continuous/checkpoints-meta/checkpoint.pth" ]; then
        CHECKPOINT_SIZE=$(du -h "experiments/${checkpoint}_ncsnpp_continuous/checkpoints-meta/checkpoint.pth" | cut -f1)
        echo "  ‚úÖ $checkpoint: $CHECKPOINT_SIZE"
    else
        echo "  ‚ö†Ô∏è $checkpoint: manquant"
    fi
done

# Dataset
if [ -d "data/afhq/train" ]; then
    TRAIN_COUNT=$(find data/afhq/train -name "*.jpg" | wc -l)
    VAL_COUNT=$(find data/afhq/val -name "*.jpg" | wc -l)
    echo "üêï Dataset AFHQ: $TRAIN_COUNT train + $VAL_COUNT val images"
else
    echo "‚ö†Ô∏è Dataset AFHQ manquant"
fi

echo ""
echo "üéâ INSTALLATION TERMIN√âE - GH200 OPTIMIS√âE !"
echo "============================================="
echo ""
echo "üöÄ Configuration finale:"
echo "   GPU: NVIDIA GH200 480GB (94GB VRAM utilisable)"
echo "   CUDA: 12.8 + Compute 9.0"
echo "   PyTorch: $(python -c 'import torch; print(torch.__version__)' 2>/dev/null || echo 'Error')"
echo "   Extensions: Compil√©es avec succ√®s"
echo "   NumPy: $(python -c 'import numpy; print(numpy.__version__)' 2>/dev/null || echo 'Error') (< 2.0 pour compatibilit√©)"
echo ""
echo "üìã Commandes de test disponibles:"
echo ""
echo "üé® SAMPLING :"
echo "   python pretrained_sampling.py church 4"
echo "   python pretrained_sampling.py celebahq_256 4"
echo "   python pretrained_sampling.py ffhq_1024 4"
echo "   python pretrained_sampling.py afhq_512 4"
echo ""
echo "üî• TRAINING AFHQ (utilise 94GB VRAM) :"
echo "   python main.py --config configs/ve/afhq_512_ncsnpp_continuous.py --workdir experiments/afhq_512_ncsnpp_continuous --mode train"
echo ""
echo "üìä MONITORING GPU pendant training:"
echo "   watch -n 1 nvidia-smi"
echo ""
echo "üí™ Avec 94GB de VRAM, vous pouvez entra√Æner des mod√®les √©normes !"
echo "‚ú® Setup termin√© - Bon training ! üöÄ"
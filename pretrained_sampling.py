#!/usr/bin/env python3
"""
Script de sampling qui r√©utilise directement les fonctions existantes du codebase.
Pas de duplication de code - utilise run_lib.py et evaluation.py directement.
"""
import sys
import torch
import numpy as np
import os
import logging
from datetime import datetime
from tqdm import tqdm
import warnings

# Configuration du logging
logging.basicConfig(level=logging.INFO, 
                   format='%(levelname)s - %(filename)s - %(asctime)s - %(message)s')

sys.path.append('.')

# Imports obligatoires pour l'enregistrement des mod√®les
from models import ncsnpp, ddpm, ncsnv2

# R√©utilisation directe des fonctions existantes
import run_lib
from models import utils as mutils
from models.ema import ExponentialMovingAverage
import sampling
import sde_lib
import datasets
import losses
from utils import restore_checkpoint
from tools.checkpoint_selector import select_afhq_checkpoint, copy_checkpoint_to_meta
from tools.seed_utils import configure_batch_seeds, apply_batch_seeds, log_seed_strategy

# ‚úÖ IMPORT DU MODULE EXTERNE
from tools.config_modifier import interactive_config_modification, print_config_summary


def print_sampling_summary(config, model_name, num_samples, checkpoint_path, new_config = False):
    """Affiche un r√©sum√© de la configuration de sampling."""
    
    logging.info("="*80)
    logging.info("EDITED SAMPLING CONFIGURATION SUMMARY")  if new_config else logging.info("SAMPLING CONFIGURATION SUMMARY")
    logging.info("="*80)
    logging.info(f"üéØ Model: {model_name}")
    logging.info(f"üìÅ Checkpoint: {checkpoint_path}")
    logging.info(f"üñºÔ∏è  Samples to generate: {num_samples}")
    logging.info(f"üìê Resolution: {config.data.image_size}x{config.data.image_size}")
    logging.info(f"üé® Sampling method: {config.sampling.method}")
    logging.info(f"üìä Predictor: {config.sampling.predictor}")
    logging.info(f"üîß Corrector: {config.sampling.corrector}")
    logging.info(f"üìà SNR: {config.sampling.snr}")
    logging.info(f"üßπ Denoising: {config.sampling.noise_removal}")
    logging.info(f" Sigma_max : {config.model.sigma_max}")
    logging.info("="*80)
    logging.info("")



def load_config(model_name):
    """Charge la configuration selon le mod√®le."""
    if model_name == 'church':
        from configs.ve import church_ncsnpp_continuous
        return church_ncsnpp_continuous.get_config()
    elif model_name == 'celebahq_256':
        from configs.ve.celebahq_256_ncsnpp_continuous import get_config as get_celebahq_256_config
        return get_celebahq_256_config()
    elif model_name == 'ffhq_1024':
        from configs.ve.ffhq_1024_ncsnpp_continuous import get_config as get_ffhq_1024_config
        return get_ffhq_1024_config()
    elif model_name == 'afhq_512':
        from configs.ve.afhq_512_ncsnpp_continuous import get_config as get_afhq_512_config
        return get_afhq_512_config()
    else:
        raise ValueError(f"Mod√®le '{model_name}' non support√©. Utilisez 'church' ou 'celebahq_256' ou 'ffhq_1024 ou afhq_512'")


def quick_sample_from_checkpoint(model_name, checkpoint_path, num_samples=8, output_dir=None, seed_mode='random'):
    """
    G√©n√®re des samples en r√©utilisant directement les fonctions de run_lib.py
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Configuration
    logging.info(f"üéØ Chargement de la configuration pour {model_name}")
    config = load_config(model_name)
    print_sampling_summary(config, model_name, num_samples, checkpoint_path)

    # ‚úÖ MODIFICATION INTERACTIVE EXTERNALIS√âE
    config_modified = interactive_config_modification(config)
    
    # ‚úÖ AFFICHAGE FINAL SI MODIFI√â
    if config_modified:
        print_sampling_summary(config, model_name, num_samples, checkpoint_path, new_config=True)

    # Dossier de sortie
    import os
    if output_dir is None:
        output_dir = f"samples/{model_name}/general/samples_{model_name}_{num_samples}imgs_{timestamp}"
        
    os.makedirs(output_dir, exist_ok=True)
    
    logging.info(f"üì¶ Cr√©ation du mod√®le...")
    
    # R√âUTILISATION: Cr√©er le mod√®le exactement comme run_lib.py
    score_model = mutils.create_model(config)
    ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)
    optimizer = losses.get_optimizer(config, score_model.parameters())
    state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0)
    
    # R√âUTILISATION: Charger le checkpoint exactement comme run_lib.py
    logging.info(f"üìä Chargement du checkpoint: {checkpoint_path}")
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            state = restore_checkpoint(checkpoint_path, state, device=config.device)
        logging.info("‚úÖ Checkpoint charg√© avec succ√®s")
        logging.info(f"   Step: {state['step']:,}")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du chargement: {e}")
        return None
    
    # R√âUTILISATION: Setup SDE exactement comme run_lib.py (lignes 76-85)
    logging.info("‚öôÔ∏è Configuration du SDE...")
    if config.training.sde.lower() == 'vpsde':
        sde = sde_lib.VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)
        sampling_eps = 1e-3
    elif config.training.sde.lower() == 'subvpsde':
        sde = sde_lib.subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)
        sampling_eps = 1e-3
    elif config.training.sde.lower() == 'vesde':
        sde = sde_lib.VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)
        sampling_eps = 1e-5
    else:
        raise NotImplementedError(f"SDE {config.training.sde} non support√©.")
    
    # R√âUTILISATION: Inverse scaler comme run_lib.py
    inverse_scaler = datasets.get_data_inverse_scaler(config)
    
    # R√âUTILISATION: Fonction de sampling comme run_lib.py
    sampling_shape = (num_samples, config.data.num_channels, config.data.image_size, config.data.image_size)
    sampling_fn = sampling.get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)
    
    logging.info("üéØ Configuration des seeds...")
    batch_seed, seeds_info = configure_batch_seeds(num_samples, seed_mode)
    log_seed_strategy(seed_mode, num_samples)
    
    # R√âUTILISATION: G√©n√©ration exactement comme run_lib.py (lignes 150-152)
    logging.info("üé® G√©n√©ration des samples...")
    logging.info(f"   M√©thode: {config.sampling.method}")
    logging.info(f"   Steps par sample: {sde.N}")

    ema.store(score_model.parameters())
    ema.copy_to(score_model.parameters())

    # ‚úÖ NOUVEAU: Application des seeds juste avant g√©n√©ration
    apply_batch_seeds(batch_seed, seeds_info)

    # Timer pour estimer le temps
    import time
    start_time = time.time()

    # ‚úÖ G√âN√âRATION BATCH (INCHANG√âE - RAPIDE)
    sample, nfe = sampling_fn(score_model)

    elapsed_time = time.time() - start_time
    ema.restore(score_model.parameters())

    logging.info(f"‚úÖ G√©n√©ration termin√©e en {elapsed_time:.1f}s")
    logging.info(f"   Temps par image: {elapsed_time/num_samples:.1f}s")
    
    logging.info(f"‚úÖ G√©n√©ration termin√©e! NFE: {nfe}")
    
    # R√âUTILISATION: Sauvegarde exactement comme run_lib.py (lignes 157-166)
    logging.info("üíæ Sauvegarde...")
    
    # Traitement identique √† run_lib.py
    from torchvision.utils import make_grid, save_image
    
    nrow = int(np.sqrt(sample.shape[0]))
    image_grid = make_grid(sample, nrow, padding=2)
    sample_np = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)
    
    # Sauvegarde grille
    base_name = f"{model_name}_{config.data.image_size}px_{timestamp}"
    grid_path = os.path.join(output_dir, f"{base_name}_grid.png")
    save_image(image_grid, grid_path)
    
    # Sauvegarde numpy
    np_path = os.path.join(output_dir, f"{base_name}_samples.npz")
    np.savez_compressed(np_path, samples=sample_np, nfe=nfe, model=model_name)
    
    # Images individuelles
    individual_dir = os.path.join(output_dir, "individual")
    os.makedirs(individual_dir, exist_ok=True)
    for i, img in enumerate(sample):
        save_image(img, os.path.join(individual_dir, f"sample_{i:03d}.png"))
    
    # Calculer la taille des fichiers
    grid_size = os.path.getsize(grid_path) / (1024*1024)  # En MB
    npz_size = os.path.getsize(np_path) / (1024*1024)

    logging.info("")
    logging.info("="*60)
    logging.info("‚ú® SAMPLING COMPLETED SUCCESSFULLY!")
    logging.info("="*60)
    logging.info(f"üìÅ Output directory: {output_dir}/")
    logging.info(f"üñºÔ∏è  Images generated: {len(sample)}")
    logging.info(f"üìê Resolution: {config.data.image_size}x{config.data.image_size}")
    logging.info(f"üî¢ NFE (function evaluations): {nfe:,}")
    logging.info(f"üìä Files saved:")
    logging.info(f"   - Grid image: {os.path.basename(grid_path)} ({grid_size:.1f} MB)")
    logging.info(f"   - NumPy archive: {os.path.basename(np_path)} ({npz_size:.1f} MB)")
    logging.info(f"   - Individual images: {individual_dir}/")
    logging.info("="*60)
    
    return output_dir


def sample_pretrained_model(model_name, num_samples=4, output_dir=None, interactive=True, seed_mode='random'):
    """
    Interface simple qui utilise les chemins de checkpoint par d√©faut avec s√©lection AFHQ.
    """
    # Chemins par d√©faut bas√©s sur la structure standard
    checkpoint_paths = {
        'church': "experiments/church_ncsnpp_continuous/checkpoints-meta/checkpoint.pth",
        'celebahq_256': "experiments/celebahq_256_ncsnpp_continuous/checkpoints-meta/checkpoint.pth", 
        'ffhq_1024': "experiments/ffhq_1024_ncsnpp_continuous/checkpoints-meta/checkpoint.pth",
        'afhq_512': "experiments/afhq_512_ncsnpp_continuous/checkpoints-meta/checkpoint.pth" 
    }
    
    if model_name not in checkpoint_paths:
        raise ValueError(f"Mod√®le '{model_name}' non support√©. Disponibles: {list(checkpoint_paths.keys())}")
    
    # NOUVEAU: S√©lection interactive pour AFHQ
    if model_name == 'afhq_512':
        selected_checkpoint = select_afhq_checkpoint(model_name, interactive=interactive)
        if selected_checkpoint is None:
            logging.error("‚ùå Aucun checkpoint AFHQ s√©lectionn√©")
            return None
        
        # Copier vers checkpoints-meta pour utilisation
        copy_checkpoint_to_meta(selected_checkpoint, model_name)
    
    checkpoint_path = checkpoint_paths[model_name]
    
    if not os.path.exists(checkpoint_path):
        logging.error(f"‚ùå Checkpoint non trouv√©: {checkpoint_path}")
        logging.info("üí° V√©rifiez que le mod√®le est bien entra√Æn√© ou sp√©cifiez un autre chemin")
        return None
    
    return quick_sample_from_checkpoint(model_name, checkpoint_path, num_samples, output_dir, seed_mode)


def main():
    """Point d'entr√©e principal avec support des seeds."""
    import argparse
    #python pretrained_sampling.py afhq_512 15 --seeds 12,22,23,43,33,233,332,38, 39, 40, 37, 209, 392, 329, 983
    parser = argparse.ArgumentParser(description='Sampling pour mod√®les pr√©-entra√Æn√©s avec seeds')
    parser.add_argument('model', choices=['church', 'celebahq_256', 'ffhq_1024', 'afhq_512'], 
                       help='Mod√®le √† utiliser')
    parser.add_argument('num_samples', type=int, default=4, nargs='?',
                       help='Nombre d\'√©chantillons (d√©faut: 4)')
    parser.add_argument('output_dir', type=str, default=None, nargs='?',
                       help='Dossier de sortie (optionnel)')
    parser.add_argument('checkpoint_path', type=str, default=None, nargs='?',
                       help='Chemin du checkpoint (optionnel)')
    
    parser.add_argument('--fixed-seeds', action='store_true', 
                       help='Utiliser des seeds fixes (reproductible) - D√âFAUT')
    parser.add_argument('--random-seeds', action='store_true',
                       help='Utiliser des seeds al√©atoires')
    parser.add_argument('--seeds', type=str,
                       help='Seeds personnalis√©s s√©par√©s par des virgules (ex: 42,123,456)')
    parser.add_argument('--auto', action='store_true',
                       help='Mode automatique (pas d\'interaction)')
    
    args = parser.parse_args()
   
    if args.fixed_seeds:            # Si --fixed-seeds ‚Üí fixe
        seed_mode = 'fixed'
    elif args.random_seeds:         # Si --random-seeds ‚Üí al√©atoire
        seed_mode = 'random'
    elif args.seeds:                # Si --seeds 42,123 ‚Üí personnalis√©  
        seed_mode = [int(s.strip()) for s in args.seeds.split(',')]
    else:                          # SINON ‚Üí al√©atoire (d√©faut option B)
        seed_mode = 'random'
    
    interactive = not args.auto
    
    # ‚úÖ AFFICHAGE DE LA CONFIGURATION
    logging.info("="*60)
    logging.info("üéØ SAMPLING CONFIGURATION")
    logging.info("="*60)
    logging.info(f"Mod√®le: {args.model}")
    logging.info(f"√âchantillons: {args.num_samples}")
    if seed_mode == 'random':
        logging.info("Seeds: AL√âATOIRES")
    elif seed_mode == 'fixed':
        logging.info("Seeds: FIXES (reproductible)")
    elif isinstance(seed_mode, list):
        logging.info(f"Seeds: PERSONNALIS√âS {seed_mode}")
    if args.auto:
        logging.info("Mode: AUTOMATIQUE")
    logging.info("="*60)
    
    # Si un checkpoint custom est fourni
    if args.checkpoint_path:
        result_dir = quick_sample_from_checkpoint(args.model, args.checkpoint_path, 
                                                 args.num_samples, args.output_dir, seed_mode)
    else:
        result_dir = sample_pretrained_model(args.model, args.num_samples, args.output_dir, 
                                           interactive, seed_mode)
    
    if result_dir:
        print(f"\n‚ú® Termin√©! R√©sultats dans: {result_dir}")
    else:
        print("\n‚ùå √âchec de la g√©n√©ration")
        sys.exit(1)

if __name__ == "__main__":
    main()
